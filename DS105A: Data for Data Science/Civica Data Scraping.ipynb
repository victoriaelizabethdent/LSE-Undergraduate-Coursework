{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DS105A: Data for Data Science: Summative 01\n",
        "## Part 1\n",
        "1. Scrap the list of events from the [Civica Schedule Page](https://socialdatascience.network/#schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "from pprint import pprint\n",
        "from scrapy import Selector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {},
      "outputs": [],
      "source": [
        "toScrape = 'https://socialdatascience.network/#schedule'\n",
        "response = requests.get(toScrape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {},
      "outputs": [],
      "source": [
        "sel = Selector(text=response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Produce a dataframe called 'df_schedule' with the columns 'title', 'speaker', 'date', and 'link'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {},
      "outputs": [],
      "source": [
        "cards = sel.css(\"div.card-body\")\n",
        "\n",
        "# collecting all the event titles\n",
        "titles = cards.css('h6 ::text').getall()\n",
        "\n",
        "# collecting all the event speakers\n",
        "datesSpeakers = cards.css(\"p ::text\").getall()\n",
        "speakers = datesSpeakers[::2]\n",
        "for i in range(len(speakers)):\n",
        "    if \"Speaker: \" not in speakers[i]:\n",
        "        speakers[i] = \"\"\n",
        "    else:\n",
        "        speakers[i] = speakers[i][9::]\n",
        "\n",
        "# collecting all the event dates and converting them to the date data type\n",
        "dates = datesSpeakers[1::2]\n",
        "for i in range(len(dates)):\n",
        "    dates[i] = dates[i][6:]\n",
        "    dates[i] = pd.to_datetime(dates[i])\n",
        "\n",
        "# collecting all the event links\n",
        "links = cards.css('a ::attr(href)').getall()\n",
        "for i in range(len(links)):\n",
        "    links[i] = 'https://socialdatascience.network/' + links[i]\n",
        "\n",
        "links = links[::2]\n",
        "# better way to remove duplicates??\n",
        "\n",
        "\n",
        "# producing a dataframe called df_schedule\n",
        "df_schedule = pd.DataFrame({\n",
        "    'Title': titles,\n",
        "    'Date': dates,\n",
        "    'Speakers': speakers,\n",
        "    'Link': links,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Save it to a CSV file named 'schedule.csv' in the data folder of your repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_schedule.to_csv('./data/schedule.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 396,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "187\n"
          ]
        }
      ],
      "source": [
        "eventLinks = []\n",
        "description = []\n",
        "allSpeakers = []\n",
        "talkSubtitles = []\n",
        "\n",
        "for i in range(len(df_schedule)):\n",
        "    newScraping = links[i]\n",
        "    response = requests.get(newScraping)\n",
        "    sel = Selector(text=response.text)\n",
        "    agenda = sel.css(\"div.col-md-10\")\n",
        "    \n",
        "    ls_h4 = agenda.css(\"h4\").getall()\n",
        "    ls_h4_text = agenda.css(\"h4 ::text\").getall()\n",
        "\n",
        "    # collecting the list of the speakers\n",
        "    speakerNames = agenda.css(\"span ::text\").getall()\n",
        "    x = 0\n",
        "    for i in range(len(ls_h4)):\n",
        "        if \"<span>\" in ls_h4[i]:\n",
        "           allSpeakers.append(speakerNames[x])\n",
        "           x += 1\n",
        "        else: \n",
        "            allSpeakers.append(\"\")\n",
        "            \n",
        "    # collecting the list of the event titles\n",
        "    for i in range(len(ls_h4_text)):\n",
        "        for i in range(len(speakerNames)):\n",
        "            if speakerNames[i] in ls_h4_text:\n",
        "                ls_h4_text.remove(speakerNames[i])\n",
        "    talkSubtitles.extend(ls_h4_text)\n",
        "\n",
        "    # collecting the list of links\n",
        "    for i in range(len(ls_h4_text)):\n",
        "        eventLinks.append(newScraping)\n",
        "\n",
        "    # collecting the descriptions of each talk\n",
        "    ls_p = agenda.css(\"p ::text\").getall()\n",
        "    ls_bold = agenda.css(\"b ::text\").getall()\n",
        "\n",
        "    for i in range(len(ls_p)):\n",
        "        for j in range(len(ls_bold)):\n",
        "            if ls_p[i] == ls_bold[j]:\n",
        "                ls_p[i] = ls_p[i] + ls_p[i + 1]\n",
        "            description.append(ls_p)\n",
        "    \n",
        "\n",
        "df_agenda = pd.DataFrame({\n",
        "    'Event Link': eventLinks,\n",
        "    'Talk Title': talkSubtitles,\n",
        "    'Talk Speaker': allSpeakers,\n",
        "    # 'Talk Description': description\n",
        "})\n",
        "\n",
        "\n",
        "df_agenda.to_csv('./data/agenda.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While coding, I realised that for certain CIVICA events, the 'Agenda' table is not consistent across the entire website. Therefore, in order to successfully scrape all the events in the desired way (i.e. to add information about the speakers, event titles, event links and descriptions to the agenda.csv file), I must account for this.\n",
        "\n",
        "This discrepancy is visible on the website: https://socialdatascience.network/polarisation/2023.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# code for this here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While attempting to gain all the descriptions, I noticed that some of the descriptions have further information about speakers, but these are not shown within \"p\" tags, but inside \"li\" ones. To fully scrape the descriptions of each talk, this must be accounted for.\n",
        "\n",
        "This is visible on the website: https://socialdatascience.network/launch.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "containsList = False\n",
        "if \"<li>\" in agenda:\n",
        "    containsList += 1\n",
        "\n",
        "print(containsList)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
