{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A London Housing Analysis\n",
    "#### Group Coursework for DS105A: Data for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Necessary Packages and Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from pprint import pprint\n",
    "from scrapy import Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property Data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined our RightMove search so we could scrape through all the properties our project need. Our initial attempt paginated through each site, as seen below, but this attempt was unsuccessful as we attempted to access each page (the ConnectionError in the following code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = \"https://www.rightmove.co.uk/house-prices/london-87490.html?\"\n",
    "# headers = {'User-Agent': 'Summative4 (Group Project)'}\n",
    "# pages = 40  # CAN THIS BE DYNAMIC -> CHANGES AS THE AMOUNT OF PAGES MIGHT OVERTIME?\n",
    "\n",
    "# for page in range(1, pages + 1):\n",
    "#     parameters = {\"soldIn\": \"5\", \"page\": str(page)}\n",
    "#     to_scrape = requests.get(base_url, params=parameters, headers=headers).url\n",
    "#     response = requests.get(to_scrape, headers)\n",
    "#     sel = Selector(text=response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a consistent error, which we attribute to RightMove's blockage of automated scraping, we changed the 'base_url' of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = \"https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=87490\"\n",
    "# headers = {'User-Agent': 'Summative4 (Group Project)'}\n",
    "# pages = 40  # CAN THIS BE DYNAMIC -> CHANGES AS THE AMOUNT OF PAGES MIGHT OVERTIME?\n",
    "\n",
    "# for page in range(1, pages + 1):\n",
    "#     parameters = {\"page\": str(page)}\n",
    "#     response = requests.get(base_url, params=parameters, headers=headers)\n",
    "#     print(response.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging JSON Files \n",
    "**(ChatGPT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def merge_and_remove_duplicates(files):\n",
    "#     merged_data = {}\n",
    "\n",
    "#     for file in files:\n",
    "#         with open(file, 'r') as f:\n",
    "#             data = json.load(f)\n",
    "#             # Merge data from the current file into the merged_data dictionary\n",
    "#             merged_data.update(data)\n",
    "\n",
    "#     # Remove duplicates by converting the merged_data dictionary to a set of tuples\n",
    "#     unique_data = {tuple(item.items()) for item in merged_data.items()}\n",
    "\n",
    "#     # Convert the set of tuples back to a dictionary\n",
    "#     result_data = dict(unique_data)\n",
    "\n",
    "#     return result_data\n",
    "\n",
    "# def save_to_file(data, output_file):\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         json.dump(data, f, indent=2)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # List of JSON files to merge\n",
    "#     input_files = ['file1.json', 'file2.json', 'file3.json']\n",
    "\n",
    "#     # Output file for the merged and deduplicated data\n",
    "#     output_file = 'merged_data.json'\n",
    "\n",
    "#     merged_data = merge_and_remove_duplicates(input_files)\n",
    "#     save_to_file(merged_data, output_file)\n",
    "\n",
    "#     print(f'Merged data has been saved to {output_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanding our Data Collection\n",
    "In searching RightMove for all 32 London boroughs and the City of London, we have increased our property dataset from having under 1000 properties, when the search was simply 'London', to over 30,000 properties. This required the tedious process of searching for each borough, the effect it will have on our analysis is immeasurable. This has been converted into a .csv file available [here]('data/secondary/boroughs.csv')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs = {\n",
    "    'Barking and Dagenham': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61400',\n",
    "    'Barnet': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93929',\n",
    "    'Bexley': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93932',\n",
    "    'Brent': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93935',\n",
    "    'Bromley': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93938',\n",
    "    'Camden': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93941',\n",
    "    'City of London': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61224',\n",
    "    'Croydon': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93944',\n",
    "    'Ealing': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93947',\n",
    "    'Enfield': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93950',\n",
    "    'Greenwich': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61226',\n",
    "    'Hackney': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93953',\n",
    "    'Hammersmith and Fulham': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61407',\n",
    "    'Haringey': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61227',\n",
    "    'Harrow': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93956',\n",
    "    'Havering': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61228',\n",
    "    'Hillingdon': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93959',\n",
    "    'Hounslow': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93962',\n",
    "    'Islington': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93965',\n",
    "    'Kensington and Chelsea': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61229',\n",
    "    'Kingston upon Thames': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93968',\n",
    "    'Lambeth': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93971',\n",
    "    'Lewisham': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61413',\n",
    "    'Merton': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61414',\n",
    "    'Newham': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61231',\n",
    "    'Redbridge': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61537',\n",
    "    'Richmond upon Thames': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61415',\n",
    "    'Southwark': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61518',\n",
    "    'Sutton': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93974',\n",
    "    'Tower Hamlets': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61417',\n",
    "    'Waltham Forest': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61232',\n",
    "    'Wandsworth': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=93977',\n",
    "    '(City of) Westminster': 'https://www.rightmove.co.uk/house-prices/result?soldIn=5&locationType=REGION&locationId=61233',\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(list(boroughs.items()), columns=['borough', 'url'])\n",
    "df.to_csv('data/secondary/boroughs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "headers = {'User-Agent': 'Summative4 (Group Project)'}\n",
    "for url in boroughs.values():\n",
    "    for page in range(40):\n",
    "        parameters = {'page': str(page)}\n",
    "        response = requests.get(url, params=parameters, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            json_data.extend(json.loads(response.text)['results']['properties'])\n",
    "\n",
    "with open(os.path.join('./data/primary/', 'json_data'), 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "df = pd.DataFrame(json_data)\n",
    "\n",
    "# fill rows with NaN in bedroom column with 1.0 (flats)\n",
    "df['bedrooms'].fillna(1.0, inplace=True)\n",
    "\n",
    "# Turn location column into two columns lat and lng\n",
    "df['lat'] = df['location'].apply(lambda x: x['lat'])\n",
    "df['lng'] = df['location'].apply(lambda x: x['lng'])\n",
    "\n",
    "# explode transactions to create a new row for every transaction on a specific property\n",
    "# Turn into new columns of display price and date sold\n",
    "df_expanded = df.explode('transactions')\n",
    "df_expanded['displayPrice'] = df_expanded['transactions'].apply(lambda x: x['displayPrice'])\n",
    "df_expanded['dateSold'] = df_expanded['transactions'].apply(lambda x: x['dateSold'])\n",
    "\n",
    "# drop transactions, location, and image columns\n",
    "df_expanded.drop('transactions', axis=1, inplace=True)\n",
    "df_expanded.drop('location', axis=1, inplace=True)\n",
    "df_expanded.drop('images', axis=1, inplace=True)\n",
    "\n",
    "# extract postcode ****NEEDS WORK***\n",
    "df_expanded['postcode'] = df_expanded['address'].str.extract(r'(\\b[A-Z]{1,2}\\d{1,2} ?\\d[A-Z]{2}\\b)')\n",
    "\n",
    "# create new column names\n",
    "new_column_names = {\n",
    "    'address': 'address',\n",
    "    'propertyType': 'property_type',\n",
    "    'bedrooms': 'num_bedrooms',\n",
    "    'hasFloorPlan': 'has_floor_plan',\n",
    "    'detailUrl': 'url',\n",
    "    'lat' : 'latitude',\n",
    "    'lng' : 'longitude',\n",
    "    'displayPrice' : 'display_price',\n",
    "    'dateSold' : 'date_sold',\n",
    "    'postcode' : 'postcode'\n",
    "}\n",
    "\n",
    "df_expanded.rename(columns=new_column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appropriate Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_expanded.info()\n",
    "\n",
    "# change variable types to ones that can be graphed in ggplot\n",
    "df_expanded['address'] = df_expanded['address'].astype(str)\n",
    "df_expanded['property_type'] = df_expanded['property_type'].astype(str)\n",
    "df_expanded['num_bedrooms'] = df_expanded['num_bedrooms'].astype(int)\n",
    "df_expanded['display_price'] = df_expanded['display_price'].replace('[\\£,]', '', regex=True).astype(int)\n",
    "df_expanded['date_sold'] = pd.to_datetime(df_expanded['date_sold'], errors='coerce')\n",
    "\n",
    "# reset index\n",
    "df_expanded = df_expanded.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Data\n",
    "***\n",
    "#### Average Property Price (by Borough)\n",
    "It would be useful to have some information on the average property price in each borough in our analysis, and if we recommend certain boroughs to different property buyers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_prices = []\n",
    "headers = {'User-Agent': 'Summative4 (Group Project)'}\n",
    "\n",
    "for url in boroughs.values():\n",
    "    parameters = {'page': str(1)}\n",
    "    response = requests.get(url, params=parameters, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        meta_tag_description = json.loads(response.text)['results']['metaTagDescription']\n",
    "        start_index = meta_tag_description.find('£')\n",
    "        end_index = meta_tag_description.find(' ', start_index)\n",
    "        average_price = meta_tag_description[start_index + 1:end_index]\n",
    "        average_price = int(average_price.replace(',', '')) \n",
    "        average_prices.append(average_price)\n",
    "\n",
    "df = pd.read_csv('data/secondary/boroughs.csv')\n",
    "df['average_price'] = average_prices\n",
    "df.to_csv('data/secondary/boroughs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crime Data\n",
    "A key factor for property buyers is the quality of the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Crime CSV\n",
    "crime_csv = pd.read_csv('data/secondary/crime.csv')\n",
    "crime_csv = crime_csv.columns.tolist()\n",
    "\n",
    "# creating a data set which contains all postcodes and the annual crimes of them \n",
    "by_postcode = pd.DataFrame(columns = ['postcode', 'crime rate'])\n",
    "\n",
    "# filling the dataset with data from the csv\n",
    "for i in range(int(len(crime_csv)/4)):\n",
    "    temp = {\"postcode\": str(crime_csv[i*4]),  \"crime rate\": str(crime_csv[i*4+2])}\n",
    "    by_postcode = pd.concat([by_postcode, pd.DataFrame([temp])], ignore_index=True)\n",
    "\n",
    "\n",
    "# clean the data\n",
    "by_postcode['postcode'] = by_postcode['postcode'].str.replace(\"'\", \"\", regex=False)\n",
    "by_postcode['crime rate'] = by_postcode['crime rate'].str.extract(r\"'(\\d+\\.\\d+|\\d+)'\").astype(int)\n",
    "by_postcode['postcode'] = by_postcode['postcode'].str.strip().str[:3]\n",
    "\n",
    "# grouping postcode's crime rate\n",
    "by_postcode = by_postcode.groupby('postcode')['crime rate'].mean().reset_index()\n",
    "\n",
    "# taking data from main data frame and turning it into new DF which has the average price for every postcode region\n",
    "columns_to_copy = ['postcode', 'display_price', 'latitude', 'longitude']\n",
    "av_price = df_expanded[columns_to_copy].copy()\n",
    "av_price['postcode'] = av_price['postcode'].astype(str)\n",
    "av_price['postcode'] = av_price['postcode'].str[:3]\n",
    "av_price = av_price.groupby('postcode')['display_price'].mean().astype(int)\n",
    "av_price = av_price.drop(av_price.index[-1])\n",
    "\n",
    "columns_to_copy = ['postcode', 'latitude']\n",
    "av_lat = df_expanded[columns_to_copy].copy()\n",
    "av_lat['postcode'] = av_lat['postcode'].astype(str)\n",
    "av_lat['postcode'] = av_lat['postcode'].str[:3]\n",
    "av_lat = av_lat.groupby('postcode')['latitude'].mean()\n",
    "av_lat = av_lat.drop(av_lat.index[-1])\n",
    "\n",
    "\n",
    "columns_to_copy = ['postcode', 'longitude']\n",
    "av_long = df_expanded[columns_to_copy].copy()\n",
    "av_long['postcode'] = av_long['postcode'].astype(str)\n",
    "av_long['postcode'] = av_long['postcode'].str[:3]\n",
    "av_long = av_long.groupby('postcode')['longitude'].mean()\n",
    "av_long = av_long.drop(av_long.index[-1])\n",
    "\n",
    "# grouping all three to create data frame with postcodes, their crime and their average price\n",
    "by_postcode = pd.merge(av_price, by_postcode, on='postcode', how='outer')\n",
    "by_postcode = pd.merge(av_lat, by_postcode, on='postcode', how='outer')\n",
    "by_postcode = pd.merge(av_long, by_postcode, on='postcode', how='outer')\n",
    "by_postcode = by_postcode.dropna()\n",
    "by_postcode = by_postcode.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Air Quality\n",
    "Air Quality is a key factor in determining where to purchase a property. It takes into account not only the areas of London which are highly polluted, but also certain (main) roads which are more polluted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading google air quality api URL and creating an empty column in data set\n",
    "url = 'https://airquality.googleapis.com/v1/history:lookup?key=AIzaSyAWS0FK9EMzGCAbnlYm9caGU8O1s4jE9N8'\n",
    "by_postcode['AQI'] = None\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# iterating through coordinates of every borough and pulling air quality\n",
    "for i in range(len(by_postcode)):\n",
    "    data = {\n",
    "        \"dateTime\": \"2024-01-28T15:00:00Z\",\n",
    "        \"location\": {\n",
    "            \"latitude\": by_postcode.iloc[i, by_postcode.columns.get_loc('latitude')],\n",
    "            \"longitude\": by_postcode.iloc[i, by_postcode.columns.get_loc('longitude')]\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "    # process the result as needed\n",
    "        aqi_value = json.dumps(result)\n",
    "        aqi_value = json.loads(aqi_value)\n",
    "        value = aqi_value['hoursInfo'][0]['indexes'][0]['aqi']\n",
    "    else:\n",
    "        value = None\n",
    "    by_postcode.loc[i, 'AQI'] = value\n",
    "\n",
    "# saving to file\n",
    "by_postcode.to_csv('Data/primary/all_postcode_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a DataFrame with Listings by Postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting columns from postcode which would conflict\n",
    "del by_postcode['latitude']\n",
    "del by_postcode['longitude']\n",
    "del by_postcode['display_price']\n",
    "\n",
    "# merging data from postcodes with data frame including every property listing\n",
    "listing_data = df_expanded.copy()\n",
    "listing_data = listing_data.drop_duplicates(subset=['address'])\n",
    "listing_data = listing_data.dropna(subset='postcode')\n",
    "listing_data['postcode'] = listing_data['postcode'].str[:3]\n",
    "listing_data = pd.merge(listing_data, by_postcode, how='inner', on='postcode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property Distance from the Centre of London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate distance using Haversine formula\n",
    "def haversine(row):\n",
    "    central_lat = 51.50313\n",
    "    central_lon = -0.150811\n",
    "\n",
    "    lat1, lon1 = row['latitude'], row['longitude']\n",
    "\n",
    "    dlat = math.radians(lat1 - central_lat)\n",
    "    dlon = math.radians(lon1 - central_lon)\n",
    "\n",
    "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(central_lat)) * math.cos(math.radians(lat1)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "\n",
    "    distance = 6371 * c  # distance in kilometers\n",
    "    return distance\n",
    "\n",
    "# apply the haversine function to the DataFrame\n",
    "listing_data['central dist'] = listing_data.apply(haversine, axis=1).round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded.to_csv('Data/secondary/df_expanded.csv')\n",
    "listing_data.to_csv('Data/primary/all_listing_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
